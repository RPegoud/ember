{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fbc9cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanpegoud/Documents/Projects/ember/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from typing import Optional\n",
    "from ember import (\n",
    "    Transformer,\n",
    "    MultiHeadLatentAttn,\n",
    "    KVCache,\n",
    "    GroupedQueryAttn,\n",
    "    Tokenizer,\n",
    "    RoPE,\n",
    "    apply_rotary_pos_emb,\n",
    "    TopKSampler\n",
    ")\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50f4092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of Transformer(\n",
      "  (embed): Embedding(50257, 512)\n",
      "  (attn_blocks): ModuleList(\n",
      "    (0-2): 3 x AttentionBlock(\n",
      "      (mlp): SwiGLU(\n",
      "        (W): Linear(in_features=512, out_features=682, bias=False)\n",
      "        (V): Linear(in_features=512, out_features=682, bias=False)\n",
      "        (W2): Linear(in_features=682, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm): RMSNorm()\n",
      "      (attn): GroupedQueryAttn(\n",
      "        (fused_qkv): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (rope): RoPE()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      ")>\n",
      "Parameter count: 3.12e+07\n"
     ]
    }
   ],
   "source": [
    "B, S, D = 8, 1024, 512\n",
    "mla_kwargs = {\n",
    "    \"latent_dim\": 256,\n",
    "    \"pos_dim\": 128,\n",
    "    \"n_heads\": 16,\n",
    "}\n",
    "gqa_kwargs = {\n",
    "    \"n_query_heads\": 8,\n",
    "    \"n_query_groups\": 4,\n",
    "}\n",
    "# \n",
    "# tk = Tokenizer()\n",
    "tk = Tokenizer(model = \"openai-community/gpt2\")\n",
    "model = Transformer(\n",
    "    vocab_size=tk.vocab_size,\n",
    "    model_dim=D,\n",
    "    hidden_dim=1024,\n",
    "    attn_module=GroupedQueryAttn,\n",
    "    attn_kwargs=gqa_kwargs,\n",
    "    n_attn_blocks=3,\n",
    ")\n",
    "\n",
    "# mini_deepseek = Transformer(\n",
    "#     vocab_size=tk.vocab_size,\n",
    "#     model_dim=D,\n",
    "#     hidden_dim=1024,\n",
    "#     attn_module=MultiHeadLatentAttn,\n",
    "#     attn_kwargs=mla_kwargs,\n",
    "#     n_attn_blocks=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3639692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = [\"Hey there\", \"How are you?\"]\n",
    "x = tk(txt)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c447dffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hey there', 'How are you?'],\n",
       " ['Hey there<|endoftext|><|endoftext|>', 'How are you?'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt, tk.decode(tk(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa68d53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey there',\n",
       " 'How are you?STDOUT decides sorted Bis========Demonberto INFORMATION shouldersbertoEP veterin shoulders cheapest Jonathanorea suppliersTro rapid overloadomiournals injust Wicked patents injustomi Vinyl addict shoulders cheapestEP giving giving periphery periphery INFORMATIONeddedTro shoulders givingEP HepDemon halls remodazz Palest addict remod Bis']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(\n",
    "    x,\n",
    "    max_new_tokens=50,\n",
    "    sampler=TopKSampler(50),\n",
    "    tokenizer=tk,\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88d5c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10814,   612, 50256, 50256, 16039,  6728,  9690,  9690, 26747, 30016,\n",
      "         9937, 37942,  4152, 30978, 24696, 22692, 19114, 33719, 21889, 24696,\n",
      "        19380, 33719,  8563, 24776, 29714, 29714,  9328, 48215,  8563, 26747,\n",
      "        15594, 30978, 24696, 32917,   701, 40292,   701, 33719, 42853, 39646,\n",
      "         9328, 19380, 19380, 38493,  9328,  9328, 30962, 37624, 35728, 46341,\n",
      "        19530, 19380, 46341, 33719, 20612])\n",
      "2\n",
      "tensor([ 2437,   389,   345,    30, 19450,  3501, 37707, 19878, 19878, 32247,\n",
      "        48486, 19019, 19387, 19878, 32371, 46215, 16880,  3501,  3501, 38045,\n",
      "        38045, 24350, 35477, 42587, 19019, 31198, 16880, 32371,  8905, 35477,\n",
      "        23243, 46116, 45949, 42956, 16547, 38044, 43426,  5480, 11232, 32371,\n",
      "        19878, 39261, 12450, 37707, 38977, 45949, 43426, 19878, 38044, 31198,\n",
      "        33396,  8905, 49809, 29583, 48486])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hey there',\n",
       " 'How are you? olive giving NIGHT PER PER VID classy injust INC PERbertoorea Tap giving giving Bis Bis hallsDemon Tune injust prominence TapbertoEPDemon sorted awarding Vinyl Wickedolphins INFORMATION Prepare Palest Jonathanberto PER Hancock shoulders NIGHT remod Vinyl Prepare PER INFORMATION prominence ReductionEPossession Flip classy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.inference_mode\n",
    "def generate(\n",
    "    model,\n",
    "    indices: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    sampler: nn.Module,\n",
    "    tokenizer: callable,\n",
    ") -> torch.Tensor:\n",
    "    cache_config: dict = model.cache_config\n",
    "    B, S = indices.shape\n",
    "\n",
    "    cache = KVCache(\n",
    "        n_layers=model.n_attn_blocks,\n",
    "        max_batch_size=B,\n",
    "        max_seq_len=S + max_new_tokens,\n",
    "        n_heads=cache_config[\"n_heads\"],\n",
    "        head_dim=cache_config[\"head_dim\"],\n",
    "    )\n",
    "    finished = torch.zeros((B,), dtype=torch.bool, device=indices.device)\n",
    "\n",
    "    logits = model.forward(indices, cache)  # prefill cache\n",
    "    cache.initialize_prefill(S)\n",
    "\n",
    "    max_tokens = S + max_new_tokens\n",
    "    while cache.current_len <= max_tokens:\n",
    "        next_tokens = sampler(logits[:, -1, :])\n",
    "        indices = torch.cat([indices, next_tokens], dim=-1)\n",
    "\n",
    "        is_eos = next_tokens.squeeze(-1) == tokenizer.eos_token_id\n",
    "        finished = finished | is_eos\n",
    "        if finished.all():\n",
    "            break\n",
    "\n",
    "        logits = model.forward(next_tokens, cache=cache)\n",
    "        cache.step()\n",
    "\n",
    "    # remove tokens past <eos>\n",
    "    output_strings = []\n",
    "    for i in range(B):\n",
    "        print(indices[i])\n",
    "        seq = indices[i].tolist()\n",
    "        try:\n",
    "            eos_idx = seq.index(tokenizer.eos_token_id)\n",
    "            print(eos_idx)\n",
    "            seq = seq[:eos_idx]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        output_strings.append(tokenizer.decode(seq))\n",
    "\n",
    "    return output_strings\n",
    "\n",
    "\n",
    "generate(\n",
    "    model,\n",
    "    x,\n",
    "    max_new_tokens=50,\n",
    "    sampler=TopKSampler(50),\n",
    "    tokenizer=tk,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98631bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end_of_text|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk = Tokenizer()\n",
    "tk.decode([tk.pad_token_id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
