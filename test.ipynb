{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24573cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from einops import rearrange\n",
    "\n",
    "from ignite.llm.rope import RoPE, apply_rotary_pos_emb\n",
    "\n",
    "\n",
    "class MultiHeadLatentAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention.\n",
    "\n",
    "    Einsum notation:\n",
    "        - `B`: batch size\n",
    "        - `S`: sequence length\n",
    "        - `NH`: number of heads\n",
    "        - `HD`: head dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_dim: int,\n",
    "        latent_dim: int,\n",
    "        pos_dim: int,\n",
    "        n_heads: int,\n",
    "        max_seq_len: int,  # TODO: define\n",
    "        rope_theta: Optional[int] = None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_dim = model_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.pos_dim = pos_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.head_dim = model_dim // n_heads\n",
    "        self.pos_head_dim = pos_dim // n_heads\n",
    "\n",
    "        # Content projections\n",
    "        self.fused_qkv_down_proj = nn.Linear(model_dim, latent_dim * 3)\n",
    "        self.fused_qkv_up_proj = nn.Linear(latent_dim * 3, model_dim * 3)\n",
    "\n",
    "        # Positional projections (decoupled RoPE)\n",
    "        self.q_pos_proj = nn.Linear(latent_dim, pos_dim)\n",
    "        self.k_pos_proj = nn.Linear(\n",
    "            model_dim, self.pos_head_dim\n",
    "        )  # project to a single pos head\n",
    "        self.fused_q_pos_proj = nn.Linear(latent_dim, pos_dim)\n",
    "\n",
    "        # Output projections\n",
    "        self.o_proj = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "        self.rope = RoPE(dim=self.pos_head_dim, base=rope_theta)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        map(init.xavier_uniform_, self.parameters())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Latent projections\n",
    "        latent_qkv = self.fused_qkv_down_proj(x)\n",
    "        latent_q, *_ = torch.split(latent_qkv, int(self.latent_dim), dim=-1)\n",
    "\n",
    "        # Up projections, reshape to multi-head\n",
    "        fused_qkv = self.fused_qkv_up_proj(latent_qkv)\n",
    "        q, k, v = torch.split(fused_qkv, int(self.model_dim), dim=-1)  # cache kv\n",
    "        q, k, v = map(\n",
    "            lambda x: rearrange(x, \"B S (NH HD) -> B NH S HD\", NH=self.n_heads),\n",
    "            (q, k, v),\n",
    "        )\n",
    "\n",
    "        # Positional Embeddings\n",
    "        pos_q = self.q_pos_proj(latent_q)\n",
    "        pos_k = self.k_pos_proj(x)\n",
    "\n",
    "        pos_q = rearrange(pos_q, \"B S (NH HD) -> B NH S HD\", NH=self.n_heads)\n",
    "        pos_k = rearrange(pos_k, \"B S (NH HD) -> B NH S HD\", NH=1)\n",
    "\n",
    "        cos, sin = self.rope(pos_q)\n",
    "        cos, sin = map(lambda x: x.transpose(0, 2), (cos, sin))\n",
    "        pos_q, pos_k = apply_rotary_pos_emb(pos_q, pos_k, cos, sin)\n",
    "        pos_k = pos_k.expand(-1, self.n_heads, -1, -1)  # broadcast to all heads\n",
    "\n",
    "        # Merge content and positional heads\n",
    "        q = torch.cat([q, pos_q], dim=-1)\n",
    "        k = torch.cat([k, pos_k], dim=-1)\n",
    "\n",
    "        # Attention and output projection\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        attn = rearrange(attn, \"B NH S HD -> B S (NH HD)\", NH=self.n_heads)\n",
    "\n",
    "        return self.o_proj(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d76db730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024, 2048])\n"
     ]
    }
   ],
   "source": [
    "B, S, D = 16, 1024, 2048\n",
    "max_seq_len = S\n",
    "x = torch.randn((B, S, D))\n",
    "latent_dim = 256\n",
    "pos_dim = 128\n",
    "n_heads = 16\n",
    "attn = MultiHeadLatentAttn(\n",
    "    D, latent_dim, pos_dim, n_heads, max_seq_len=S, rope_theta=50_000\n",
    ")\n",
    "a = attn(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
