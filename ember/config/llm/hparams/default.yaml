optimizer:
  lr: 5e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95

trainer:
  max_epochs: 1
  precision: "bf16-mixed"
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4
  log_every_n_steps: 10 
  limit_train_batches: 500
  limit_val_batches: 50
  val_check_interval: 500

callbacks:
  generate:
    max_new_tokens: 50

data:
  dataset: "roneneldan/TinyStories"
  batch_size: 64
  num_workers: 4

scheduler:
  warmup_steps: 1e3
  max_steps: 5e4
