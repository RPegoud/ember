{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67e317ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def linear_cross_entropy_fwd_kernel(\n",
    "    X_ptr,\n",
    "    X_row_stride,\n",
    "    W_ptr,\n",
    "    W_row_stride,\n",
    "    W_col_stride,\n",
    "    Y_ptr,\n",
    "    dX_ptr,\n",
    "    dX_row_stride,\n",
    "    dW_ptr,\n",
    "    dW_row_stride,\n",
    "    dW_col_stride,\n",
    "    LSE_ptr, # TODO: we don't need to store the LSE\n",
    "    Loss_ptr,\n",
    "    D: tl.constexpr,\n",
    "    V: tl.constexpr,\n",
    "    D_BLOCK: tl.constexpr,\n",
    "    V_BLOCK: tl.constexpr,\n",
    "    ignore_index: tl.constexpr,\n",
    "):\n",
    "    # --- Setup ---\n",
    "    row_id = tl.program_id(axis=0).to(tl.int64)\n",
    "    d_tile_offs = tl.arange(0, D_BLOCK)\n",
    "    v_tile_offs = tl.arange(0, V_BLOCK)\n",
    "\n",
    "    m = -float(\"inf\")  # running max\n",
    "    d = 0.0  # running exp sum\n",
    "\n",
    "    # --- Pointer Logic ---\n",
    "    X_ptr += row_id * X_row_stride\n",
    "    dX_ptr += row_id * dX_row_stride\n",
    "    Y_ptr += row_id\n",
    "    LSE_ptr += row_id\n",
    "    Loss_ptr += row_id\n",
    "\n",
    "    # --- Pre-compute the Y logit ---\n",
    "    Y = tl.load(pointer=Y_ptr)\n",
    "\n",
    "    # if Y should be ignored, zero the loss and return early\n",
    "    if Y == ignore_index:\n",
    "        tl.store(Loss_ptr, 0.0)\n",
    "        tl.store(LSE_ptr, 0.0)\n",
    "        return\n",
    "\n",
    "    Y_logit_acc = 0.0\n",
    "\n",
    "    # load the X and W tile, accumulate their dot product\n",
    "    for d_idx in tl.range(0, D, D_BLOCK):\n",
    "        d_tile_mask = (d_idx + d_tile_offs) < D\n",
    "        X_tile = tl.load(\n",
    "            X_ptr + d_idx + d_tile_offs, mask=d_tile_mask, other=0.0\n",
    "        )\n",
    "\n",
    "        W_target_tile_ptr = (\n",
    "            W_ptr + (d_idx + d_tile_offs) * W_row_stride + Y * W_col_stride\n",
    "        )\n",
    "        W_target_tile = tl.load(W_target_tile_ptr, mask=d_tile_mask, other=0.0)\n",
    "\n",
    "        Y_logit_acc += tl.sum(X_tile.to(tl.float32) * W_target_tile.to(tl.float32))\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    # iterate through X by tiles and W by blocks\n",
    "    # compute X@W, accumulate the LSE and compute the max\n",
    "    for v_idx in tl.range(0, V, V_BLOCK):\n",
    "        W_block_ptr = tl.make_block_ptr(\n",
    "            base=W_ptr,\n",
    "            shape=(D, V),\n",
    "            strides=(W_row_stride, W_col_stride),\n",
    "            offsets=(0, v_idx),\n",
    "            block_shape=(D_BLOCK, V_BLOCK),\n",
    "            order=(0, 1),\n",
    "        )\n",
    "\n",
    "        # compute X@W and accumulate the Y logit\n",
    "        acc = tl.zeros((1, V_BLOCK), dtype=tl.float32)\n",
    "        for d_idx in tl.range(0, D, D_BLOCK):\n",
    "            tile_offs = d_idx + d_tile_offs\n",
    "            tile_mask = tile_offs < D\n",
    "\n",
    "            X_tile = tl.load(X_ptr + tile_offs, mask=tile_mask, other=0.0)\n",
    "            W_block = tl.load(W_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "            acc += tl.dot(X_tile[None, :], W_block)\n",
    "\n",
    "            W_block_ptr = W_block_ptr.advance((D_BLOCK, 0))\n",
    "\n",
    "        # update LSE and max\n",
    "        m_tile = tl.max(acc)\n",
    "        new_m = tl.maximum(m, m_tile)\n",
    "        d = d * tl.exp(m - new_m) + tl.sum(tl.exp(acc - new_m))\n",
    "        m = new_m\n",
    "\n",
    "    lse = m + tl.log(d)\n",
    "    loss = lse - Y_logit_acc\n",
    "\n",
    "    tl.store(pointer=Loss_ptr, value=loss)\n",
    "    tl.store(pointer=LSE_ptr, value=lse)\n",
    "\n",
    "    # --- Backward Pass ---\n",
    "\n",
    "    # # 1: Compute the normalised probabilities P\n",
    "    for v_idx in tl.range(0, V, V_BLOCK):\n",
    "        W_block_ptr = tl.make_block_ptr(\n",
    "            base=W_ptr,\n",
    "            shape=(D, V),\n",
    "            strides=(W_row_stride, W_col_stride),\n",
    "            offsets=(0, v_idx),\n",
    "            block_shape=(D_BLOCK, V_BLOCK),\n",
    "            order=(0, 1),\n",
    "        )\n",
    "\n",
    "        P_tile = tl.zeros((1, V_BLOCK), dtype=tl.float32)\n",
    "        for d_idx in tl.range(0, D, D_BLOCK):\n",
    "            tile_offs = d_idx + d_tile_offs\n",
    "            tile_mask = tile_offs < D\n",
    "\n",
    "            X_tile = tl.load(X_ptr + tile_offs, mask=tile_mask, other=0.0)\n",
    "            W_block = tl.load(W_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "            P_tile += tl.dot(X_tile[None, :].to(tl.float32), W_block.to(tl.float32))\n",
    "\n",
    "            W_block_ptr = W_block_ptr.advance((D_BLOCK, 0))\n",
    "\n",
    "        P_tile = tl.exp(P_tile - lse)  # normalise\n",
    "        P_tile -= tl.where(\n",
    "            v_idx + v_tile_offs == Y, 1, 0\n",
    "        )  # subtract 1 when logit = label\n",
    "\n",
    "        # 2: Compute the gradients:\n",
    "        # dX = (P - Y) . W^T\n",
    "        # dW = X^T . (P - Y)\n",
    "        W_T_block_ptr = tl.make_block_ptr(  # logically transpose W\n",
    "            base=W_ptr,\n",
    "            shape=(V, D),\n",
    "            strides=(W_col_stride, W_row_stride),\n",
    "            offsets=(v_idx, 0),\n",
    "            block_shape=(V_BLOCK, D_BLOCK),\n",
    "            order=(1, 0),  # W is row major => W^T is column major\n",
    "        )\n",
    "\n",
    "        for d_idx in tl.range(0, D, D_BLOCK):\n",
    "            # 2.1: Accumulate dX\n",
    "            tile_offs = d_idx + d_tile_offs\n",
    "            tile_mask = tile_offs < D\n",
    "\n",
    "            W_T_block = tl.load(\n",
    "                W_T_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"\n",
    "            )\n",
    "\n",
    "            dX_partial = tl.dot(P_tile, W_T_block).reshape(D_BLOCK)\n",
    "            tl.atomic_add(\n",
    "                pointer=dX_ptr + d_idx + d_tile_offs,\n",
    "                val=dX_partial,\n",
    "                sem=\"relaxed\",  # the order of adds across threads does not matter\n",
    "            )\n",
    "\n",
    "            W_T_block_ptr = W_T_block_ptr.advance((0, D_BLOCK))\n",
    "\n",
    "            # 2.2: Accumulate dW\n",
    "            dW_tile_ptr = dW_ptr + (\n",
    "                (d_idx + d_tile_offs)[:, None] * dW_row_stride\n",
    "                + (v_idx + v_tile_offs)[None, :] * dW_col_stride\n",
    "            )\n",
    "\n",
    "            dW_mask = ((d_idx + d_tile_offs)[:, None] < D) & (\n",
    "                (v_idx + v_tile_offs)[None, :] < V\n",
    "            )\n",
    "\n",
    "            X_T_tile = tl.load(X_ptr + tile_offs, mask=tile_mask, other=0.0)[:, None]\n",
    "\n",
    "            dW_partial = X_T_tile.to(tl.float32) * P_tile\n",
    "            tl.atomic_add(\n",
    "                pointer=dW_tile_ptr, val=dW_partial, mask=dW_mask, sem=\"relaxed\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51c639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Match!\n",
      "dX Match!\n",
      "dW Match!\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "N, D, V = 64, 512, 1024\n",
    "X = torch.randn((N, D), device=device, requires_grad=True)\n",
    "W = torch.randn((D, V), device=device, requires_grad=True)\n",
    "Y = torch.randint(0, V, size=(N,), device=device)\n",
    "\n",
    "logits_py = X @ W\n",
    "loss_py = F.cross_entropy(logits_py, Y, reduction=\"none\")\n",
    "\n",
    "loss_py.sum().backward()\n",
    "\n",
    "expected_dX = X.grad\n",
    "expected_dW = W.grad\n",
    "\n",
    "dX_triton = torch.zeros((N, D), device=device)\n",
    "dW_triton = torch.zeros((D, V), device=device, dtype=torch.float32)\n",
    "LSE_triton = torch.zeros((N,), device=device)\n",
    "Loss_triton = torch.zeros((N,), device=device)\n",
    "\n",
    "D_BLOCK = 64\n",
    "V_BLOCK = 64\n",
    "\n",
    "linear_cross_entropy_fwd_kernel[(N,)](\n",
    "    X_ptr=X,\n",
    "    X_row_stride=X.stride(0),\n",
    "    W_ptr=W,\n",
    "    W_row_stride=W.stride(0),\n",
    "    W_col_stride=W.stride(1),\n",
    "    Y_ptr=Y,\n",
    "    dX_ptr=dX_triton,\n",
    "    dX_row_stride=dX_triton.stride(0),\n",
    "    dW_ptr=dW_triton,\n",
    "    dW_row_stride=dW_triton.stride(0),\n",
    "    dW_col_stride=dW_triton.stride(1),\n",
    "    LSE_ptr=LSE_triton,\n",
    "    Loss_ptr=Loss_triton,\n",
    "    D=D,\n",
    "    V=V,\n",
    "    D_BLOCK=D_BLOCK,\n",
    "    V_BLOCK=V_BLOCK,\n",
    "    ignore_index=-100,\n",
    ")\n",
    "\n",
    "torch.testing.assert_close(Loss_triton, loss_py, atol=1e-7, rtol=1e-5)\n",
    "print(\"Loss Match!\")\n",
    "\n",
    "torch.testing.assert_close(dX_triton, expected_dX, atol=1e-4, rtol=1e-5)\n",
    "print(\"dX Match!\")\n",
    "\n",
    "torch.testing.assert_close(dW_triton, expected_dW, atol=1e-4, rtol=1e-5)\n",
    "print(\"dW Match!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
